{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f49925c5",
   "metadata": {},
   "source": [
    "# EpiHelix AI Services (Kaggle GPU)\n",
    "\n",
    "**GPU-accelerated AI microservices for EpiHelix backend**\n",
    "\n",
    "## Setup Requirements\n",
    "\n",
    "1. ‚úÖ **Enable GPU** in Kaggle notebook settings (P100 recommended)\n",
    "2. ‚úÖ **Enable Internet** in settings\n",
    "3. ‚úÖ **Get ngrok auth token** from https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "4. ‚úÖ **Add token to Kaggle Secrets**:\n",
    "   - Go to notebook settings ‚Üí Add-ons ‚Üí Secrets\n",
    "   - Click \"Add a new secret\"\n",
    "   - Name: `NGROK_AUTH_TOKEN`\n",
    "   - Value: Your token from ngrok dashboard\n",
    "   \n",
    "Optional: Add `BACKEND_API_KEY` secret for authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d691da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (takes ~2 min first time)\n",
    "!pip install -q fastapi uvicorn pyngrok sentence-transformers transformers torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30de45d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ngrok auth token from Kaggle Secrets\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "try:\n",
    "    ngrok_token = UserSecretsClient().get_secret(\"NGROK_AUTH_TOKEN\")\n",
    "    \n",
    "    # Configure ngrok\n",
    "    from pyngrok import ngrok, conf\n",
    "    conf.get_default().auth_token = ngrok_token\n",
    "    \n",
    "    print(\"‚úÖ ngrok authentication configured successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Could not get NGROK_AUTH_TOKEN from secrets: {e}\")\n",
    "    print(\"   Please add your ngrok token to Kaggle Secrets:\")\n",
    "    print(\"   1. Go to https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
    "    print(\"   2. Copy your token\")\n",
    "    print(\"   3. Add to Kaggle: Settings ‚Üí Add-ons ‚Üí Secrets ‚Üí Add 'NGROK_AUTH_TOKEN'\")\n",
    "    print(\"\\n   Without this, ngrok tunnels may be rate-limited or fail.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08daf5c",
   "metadata": {},
   "source": [
    "## 0. Setup ngrok Authentication\n",
    "\n",
    "Get your ngrok auth token and add it to Kaggle Secrets as `NGROK_AUTH_TOKEN`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1be165",
   "metadata": {},
   "source": [
    "## 1. Load All Models on GPU\n",
    "\n",
    "Load all 3 models into GPU memory once at startup:\n",
    "- **Reranker**: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
    "- **Embedder**: sentence-transformers/all-MiniLM-L6-v2\n",
    "- **LLM (Shared)**: Qwen/Qwen2.5-3B-Instruct (used for both summarization AND chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f7d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import gc\n",
    "\n",
    "print(\"üî• CUDA available:\", torch.cuda.is_available())\n",
    "print(\"üìä GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "# Force GPU device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîÑ Loading Reranker Model (ms-marco-MiniLM-L-6-v2)...\")\n",
    "print(\"=\"*60)\n",
    "reranker_model = CrossEncoder(\n",
    "    'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "    max_length=512,\n",
    "    device=device\n",
    ")\n",
    "print(\"‚úÖ Reranker loaded\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç Loading Embedding Model (all-MiniLM-L6-v2)...\")\n",
    "print(\"=\"*60)\n",
    "embedding_model = SentenceTransformer(\n",
    "    'sentence-transformers/all-MiniLM-L6-v2',\n",
    "    device=device\n",
    ")\n",
    "print(\"‚úÖ Embedder loaded\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí¨ Loading LLM (Qwen2.5-3B-Instruct - Shared for Summarization & Chat)...\")\n",
    "print(\"=\"*60)\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    torch_dtype=torch.float16,  # Use FP16 to save memory\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=llm_model,\n",
    "    tokenizer=llm_tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    device=device\n",
    ")\n",
    "print(\"‚úÖ LLM loaded (serves both /summarize and /chat endpoints)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ All models loaded successfully!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nüìä GPU Memory: {torch.cuda.memory_allocated(0) / 1e9:.2f}GB / {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d692c968",
   "metadata": {},
   "source": [
    "## 2. FastAPI Service with 4 Endpoints\n",
    "\n",
    "Create REST API endpoints for each AI service:\n",
    "- `/rerank` - Cross-encoder reranking\n",
    "- `/embed` - Sentence embeddings\n",
    "- `/summarize` - Text summarization (Qwen Instruct)\n",
    "- `/chat` - Chatbot conversations (Same Qwen Instruct model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb177a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException, Header\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List\n",
    "import asyncio\n",
    "from functools import partial\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"EpiHelix AI Services\",\n",
    "    description=\"GPU-accelerated reranking, embedding, and LLM services\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Optional API key authentication (if you set BACKEND_API_KEY secret in Kaggle)\n",
    "API_KEY = None  # Set to your secret if needed\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# API_KEY = UserSecretsClient().get_secret(\"BACKEND_API_KEY\")\n",
    "\n",
    "def verify_api_key(authorization: Optional[str] = Header(None)):\n",
    "    \"\"\"Verify API key if configured.\"\"\"\n",
    "    if API_KEY and authorization != f\"Bearer {API_KEY}\":\n",
    "        raise HTTPException(status_code=401, detail=\"Invalid API key\")\n",
    "\n",
    "\n",
    "# ===== REQUEST/RESPONSE MODELS =====\n",
    "\n",
    "class RerankRequest(BaseModel):\n",
    "    query: str = Field(..., description=\"Search query\")\n",
    "    documents: List[str] = Field(..., description=\"List of documents to rerank\")\n",
    "    top_k: int = Field(20, description=\"Number of top results to return\")\n",
    "\n",
    "class RerankResult(BaseModel):\n",
    "    index: int\n",
    "    score: float\n",
    "\n",
    "class RerankResponse(BaseModel):\n",
    "    results: List[RerankResult]\n",
    "    model: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "\n",
    "class EmbedRequest(BaseModel):\n",
    "    texts: List[str] = Field(..., description=\"Texts to embed\")\n",
    "    normalize: bool = Field(True, description=\"Normalize embeddings to unit length\")\n",
    "\n",
    "class EmbedResponse(BaseModel):\n",
    "    embeddings: List[List[float]]\n",
    "    dimension: int\n",
    "    model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "class SummarizeRequest(BaseModel):\n",
    "    text: str = Field(..., description=\"Text to summarize\")\n",
    "    max_length: int = Field(150, description=\"Maximum summary length in tokens\")\n",
    "    temperature: float = Field(0.7, ge=0.0, le=2.0)\n",
    "\n",
    "class SummarizeResponse(BaseModel):\n",
    "    summary: str\n",
    "    model: str = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    messages: List[dict] = Field(..., description=\"Chat messages (role + content)\")\n",
    "    temperature: float = Field(0.7, ge=0.0, le=2.0)\n",
    "    max_tokens: int = Field(512, ge=1, le=2048)\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    response: str\n",
    "    model: str = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "\n",
    "# ===== ENDPOINTS =====\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"Health check endpoint.\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"services\": [\"rerank\", \"embed\", \"summarize\", \"chat\"],\n",
    "        \"gpu\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n",
    "        \"models\": {\n",
    "            \"reranker\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "            \"embedder\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            \"llm\": \"Qwen/Qwen2.5-3B-Instruct (shared for summarize & chat)\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    \"\"\"Detailed health check.\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"gpu_available\": torch.cuda.is_available(),\n",
    "        \"gpu_memory_used_gb\": torch.cuda.memory_allocated(0) / 1e9 if torch.cuda.is_available() else 0,\n",
    "        \"models_loaded\": True\n",
    "    }\n",
    "\n",
    "@app.post(\"/rerank\", response_model=RerankResponse)\n",
    "async def rerank(request: RerankRequest):\n",
    "    \"\"\"\n",
    "    Rerank documents using cross-encoder model.\n",
    "    \n",
    "    Returns documents sorted by relevance score (highest first).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create query-document pairs\n",
    "        pairs = [[request.query, doc] for doc in request.documents]\n",
    "        \n",
    "        # Run reranking in thread pool (blocking operation)\n",
    "        loop = asyncio.get_event_loop()\n",
    "        scores = await loop.run_in_executor(\n",
    "            None,\n",
    "            partial(reranker_model.predict, pairs)\n",
    "        )\n",
    "        \n",
    "        # Sort by score descending\n",
    "        ranked_indices = sorted(\n",
    "            enumerate(scores),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )[:request.top_k]\n",
    "        \n",
    "        results = [\n",
    "            RerankResult(index=idx, score=float(score))\n",
    "            for idx, score in ranked_indices\n",
    "        ]\n",
    "        \n",
    "        return RerankResponse(results=results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Reranking failed: {str(e)}\")\n",
    "\n",
    "@app.post(\"/embed\", response_model=EmbedResponse)\n",
    "async def embed(request: EmbedRequest):\n",
    "    \"\"\"\n",
    "    Generate embeddings for texts.\n",
    "    \n",
    "    Returns 384-dimensional vectors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Run embedding in thread pool\n",
    "        loop = asyncio.get_event_loop()\n",
    "        embeddings = await loop.run_in_executor(\n",
    "            None,\n",
    "            partial(\n",
    "                embedding_model.encode,\n",
    "                request.texts,\n",
    "                normalize_embeddings=request.normalize,\n",
    "                convert_to_numpy=True\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return EmbedResponse(\n",
    "            embeddings=embeddings.tolist(),\n",
    "            dimension=embeddings.shape[1]\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Embedding failed: {str(e)}\")\n",
    "\n",
    "@app.post(\"/summarize\", response_model=SummarizeResponse)\n",
    "async def summarize(request: SummarizeRequest):\n",
    "    \"\"\"\n",
    "    Summarize text using Qwen Instruct model.\n",
    "    \n",
    "    Optimized for concise summaries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create summarization prompt\n",
    "        prompt = f\"\"\"Summarize the following text concisely in {request.max_length} tokens or less:\n",
    "\n",
    "{request.text}\n",
    "\n",
    "Summary:\"\"\"\n",
    "        \n",
    "        # Generate summary in thread pool (using shared LLM)\n",
    "        loop = asyncio.get_event_loop()\n",
    "        result = await loop.run_in_executor(\n",
    "            None,\n",
    "            partial(\n",
    "                llm_pipeline,\n",
    "                prompt,\n",
    "                max_new_tokens=request.max_length,\n",
    "                temperature=request.temperature,\n",
    "                return_full_text=False\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        summary_text = result[0][\"generated_text\"].strip()\n",
    "        \n",
    "        return SummarizeResponse(summary=summary_text)\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Summarization failed: {str(e)}\")\n",
    "\n",
    "@app.post(\"/chat\", response_model=ChatResponse)\n",
    "async def chat(request: ChatRequest):\n",
    "    \"\"\"\n",
    "    Generate chat response using Qwen Instruct model (same as summarizer).\n",
    "    \n",
    "    Expects messages in format: [{\"role\": \"user\", \"content\": \"...\"}]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Format messages into prompt (Qwen chat template)\n",
    "        prompt = \"\"\n",
    "        for msg in request.messages:\n",
    "            role = msg.get(\"role\", \"user\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            if role == \"system\":\n",
    "                prompt += f\"<|im_start|>system\\n{content}<|im_end|>\\n\"\n",
    "            elif role == \"user\":\n",
    "                prompt += f\"<|im_start|>user\\n{content}<|im_end|>\\n\"\n",
    "            elif role == \"assistant\":\n",
    "                prompt += f\"<|im_start|>assistant\\n{content}<|im_end|>\\n\"\n",
    "        \n",
    "        prompt += \"<|im_start|>assistant\\n\"\n",
    "        \n",
    "        # Generate response in thread pool (using shared LLM)\n",
    "        loop = asyncio.get_event_loop()\n",
    "        result = await loop.run_in_executor(\n",
    "            None,\n",
    "            partial(\n",
    "                llm_pipeline,\n",
    "                prompt,\n",
    "                max_new_tokens=request.max_tokens,\n",
    "                temperature=request.temperature,\n",
    "                return_full_text=False\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        response_text = result[0][\"generated_text\"].strip()\n",
    "        \n",
    "        return ChatResponse(response=response_text)\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Chat generation failed: {str(e)}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ FastAPI app created with 4 endpoints:\")\n",
    "print(\"   POST /rerank    - Cross-encoder reranking\")\n",
    "print(\"   POST /embed     - Sentence embeddings\")\n",
    "print(\"   POST /summarize - Text summarization (Qwen Instruct)\")\n",
    "print(\"   POST /chat      - Chatbot (Qwen Instruct)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19984863",
   "metadata": {},
   "source": [
    "## 3. Start Server with ngrok Tunnel\n",
    "\n",
    "Expose the API publicly via ngrok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2f4b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "import time\n",
    "\n",
    "# Start ngrok tunnel\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üåê Starting ngrok tunnel...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tunnel = ngrok.connect(8000)\n",
    "public_url = tunnel.public_url  # Extract the URL string from NgrokTunnel object\n",
    "\n",
    "print(f\"\\n‚úÖ PUBLIC URL: {public_url}\")\n",
    "print(f\"üìù Add this to your backend .env file:\")\n",
    "print(f\"\\n   KAGGLE_AI_ENDPOINT={public_url}\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Start FastAPI server in background thread\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "\n",
    "server_thread = Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"\\n‚è≥ Waiting for server to start...\")\n",
    "time.sleep(5)  # Increased wait time for server startup\n",
    "\n",
    "print(\"\\nüéâ Server is running!\")\n",
    "print(f\"   Health check: {public_url}/health\")\n",
    "print(f\"   API docs: {public_url}/docs\")\n",
    "print(f\"\\nüí° Test the server by running the next cell\")\n",
    "print(\"‚ö†Ô∏è  Keep this notebook running to maintain the service\")\n",
    "print(\"   (Kaggle sessions last ~12 hours, then you need to restart)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28d01bf",
   "metadata": {},
   "source": [
    "## 4. Test Endpoints (Optional)\n",
    "\n",
    "Quick tests to verify all services work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd9b064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Wait a bit more to ensure server is ready\n",
    "print(\"‚è≥ Waiting for server to be fully ready...\")\n",
    "time.sleep(3)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üß™ Testing all endpoints...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: Health check\n",
    "print(\"\\n1Ô∏è‚É£ Health Check:\")\n",
    "try:\n",
    "    response = requests.get(f\"{public_url}/health\", timeout=10)\n",
    "    print(json.dumps(response.json(), indent=2))\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"üí° Make sure the previous cell (server startup) has finished running!\")\n",
    "\n",
    "# Test 2: Reranking\n",
    "print(\"\\n2Ô∏è‚É£ Reranking Test:\")\n",
    "try:\n",
    "    rerank_data = {\n",
    "        \"query\": \"COVID-19 cases in USA\",\n",
    "        \"documents\": [\n",
    "            \"Total COVID cases in United States reached 100 million\",\n",
    "            \"France reported new influenza outbreak\",\n",
    "            \"USA vaccination rates increased to 70%\"\n",
    "        ],\n",
    "        \"top_k\": 3\n",
    "    }\n",
    "    response = requests.post(f\"{public_url}/rerank\", json=rerank_data, timeout=30)\n",
    "    print(json.dumps(response.json(), indent=2))\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Test 3: Embeddings\n",
    "print(\"\\n3Ô∏è‚É£ Embedding Test:\")\n",
    "try:\n",
    "    embed_data = {\n",
    "        \"texts\": [\"COVID-19 pandemic\", \"Influenza outbreak\"],\n",
    "        \"normalize\": True\n",
    "    }\n",
    "    response = requests.post(f\"{public_url}/embed\", json=embed_data, timeout=30)\n",
    "    result = response.json()\n",
    "    print(f\"Generated {len(result['embeddings'])} embeddings of dimension {result['dimension']}\")\n",
    "    print(f\"First embedding (truncated): {result['embeddings'][0][:5]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Test 4: Summarization\n",
    "print(\"\\n4Ô∏è‚É£ Summarization Test:\")\n",
    "try:\n",
    "    summarize_data = {\n",
    "        \"text\": \"The COVID-19 pandemic has affected millions worldwide. Countries implemented lockdowns, mask mandates, and vaccination programs. The virus spread rapidly through communities, overwhelming healthcare systems. Scientists developed multiple vaccines in record time. Global cooperation was essential in fighting the pandemic.\",\n",
    "        \"max_length\": 50,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    response = requests.post(f\"{public_url}/summarize\", json=summarize_data, timeout=60)\n",
    "    print(json.dumps(response.json(), indent=2))\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Test 5: Chat\n",
    "print(\"\\n5Ô∏è‚É£ Chat Test:\")\n",
    "try:\n",
    "    chat_data = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is a pandemic?\"}\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 100\n",
    "    }\n",
    "    response = requests.post(f\"{public_url}/chat\", json=chat_data, timeout=60)\n",
    "    print(json.dumps(response.json(), indent=2))\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ All tests completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b024b6d",
   "metadata": {},
   "source": [
    "## 5. Keep Alive (Run Forever)\n",
    "\n",
    "Keep the server running. The notebook will stay active as long as this cell runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff9d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Server is running. Press interrupt (‚ñ†) to stop.\")\n",
    "print(f\"   Public URL: {public_url}\")\n",
    "print(f\"   API Docs: {public_url}/docs\")\n",
    "print(f\"   Health: {public_url}/health\")\n",
    "\n",
    "# Keep alive loop\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(60)\n",
    "        # Optional: print heartbeat every minute\n",
    "        if torch.cuda.is_available():\n",
    "            memory_used = torch.cuda.memory_allocated(0) / 1e9\n",
    "            print(f\"üíì Heartbeat - GPU Memory: {memory_used:.2f}GB\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nüõë Server stopped\")\n",
    "    ngrok.disconnect(public_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1147a3d4",
   "metadata": {},
   "source": [
    "## üìö Usage Instructions\n",
    "\n",
    "### Backend Integration\n",
    "\n",
    "Add to your `backend/.env`:\n",
    "```bash\n",
    "# Kaggle AI Services\n",
    "KAGGLE_AI_ENDPOINT=https://xxxx-xx-xxx-xxx-xx.ngrok-free.app\n",
    "\n",
    "# Enable AI features\n",
    "RERANKER_PROVIDER=kaggle\n",
    "EMBEDDER_PROVIDER=kaggle\n",
    "LLM_PROVIDER=kaggle\n",
    "```\n",
    "\n",
    "### Python Client Example\n",
    "\n",
    "```python\n",
    "import httpx\n",
    "\n",
    "async def rerank_with_kaggle(query: str, documents: list[str]):\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.post(\n",
    "            f\"{KAGGLE_ENDPOINT}/rerank\",\n",
    "            json={\"query\": query, \"documents\": documents, \"top_k\": 20}\n",
    "        )\n",
    "        return response.json()\n",
    "```\n",
    "\n",
    "### Maintenance\n",
    "\n",
    "- **Session Duration**: ~12 hours max\n",
    "- **Weekly Quota**: 30 GPU hours\n",
    "- **Restart**: Just click \"Run All\" again to get new ngrok URL\n",
    "- **Monitoring**: Check `/health` endpoint for status"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
